- type: replace
  path: /instance_groups/name=logstash/properties/logstash/conf?/xpack.management.pipeline.id?/-
  value:
    "cf-platform-es"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env?/DST_HOSTS?
  value:
    "((es-host))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/ES_USER?
  value:
    "((es-user))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/ES_PASSWORD?
  value:
    "((es-password))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/ES_INDEX_PREFIX?
  value:
    "((es-index_prefix))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/ES_VERSION?
  value:
    "((es-version))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/CF_API?
  value:
    "((cf-api))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/SOURCE_ENV?
  value:
    "((source-env))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/SOURCE_PLATFORM?
  value:
    "((source-platform))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/pipelines?
  value:
    - name: cf-platform-es
      config:
        filter-00-prefiltering: |
          ##-------------
          # Pre filtering
          ##-------------
          
          filter {
              mutate {
                  # Replace the unicode empty character \u0000 with ""
                  gsub => [ "message", '\u0000', ""]
          
                  # Trim excess whitespace
                  strip => [ "message" ]
              }
          
              # Drop empty useless logs
              if [message] =~ /^\s*$/ {
                  drop { }
              }
          
              mutate {
                  rename => { "message" => "@message" }
          
                  # Add tags to track which job processed this event
                  add_field => {
                          "[@parser][job]" => "${JOB_FULL_AZ_DEPLOYMENT}"
                          "[@parser][instance]" => "${INSTANCE_ID}"
                          "[@parser][az]" => "${JOB_AZ}"
                          "[@parser][name]" => "${JOB_NAME}"
                          "[@parser][index]" => "${JOB_INDEX}"
                          "[@parser][deployment]" => "${DEPLOYMENT_NAME}"
                          "[@parser][lb-host]" => "%{host}"
                          "[@parser][lb-port]" => "%{port}"
                  }
          
                  # When behind LB, this is always the IP of the haproxy, not the IP of the actual host sending the data.
                  # Remove it to avoid confusion
                  remove_field => [ "host", "port" ]
              }
          }
        filter-10-syslog_rfc5424_parsing: |
          filter {
              # Parse RFC5424 syslog messages
              grok {
                break_on_match => true
                match => {
                  "@message" => [
                    "%{SYSLOG5424LINE}",
                    "%{SYSLOG5424BASE}"
                    ]
                  }
              }
          
              mutate {
                remove_field => [ "message", "host" ]
                add_tag => [ "syslog5424" ]
              }
          
              mutate {
                # Use a friendlier naming scheme
                rename => {
                  "syslog5424_app"  => "program"
                  "syslog5424_msg" => "message"
                  "syslog5424_host" => "host"
                  "syslog5424_pri" => "priority"
                }
                remove_field => [ "syslog5424_ver", "syslog5424_proc" ]
              }
          
              # Parse JSON from message field
              json {
                skip_on_invalid_json => true  # only parse if the field actually contains JSON
                source => "message"
                target => "replace_me"  # this value will be replaced with the value of the 'program' field in the next block
                add_tag => [ "json_message" ]
                remove_field => "message"
              }
              mutate {
                rename => { "replace_me" => "%{program}" }
              }
          
              if "json_message" in [tags] {
                mutate {
                  rename => [ "[json_message][message]", "message" ]
                }
          
                date {
                  match => [ "[json_message][timestamp]", "UNIX" ]
                  target => "@timestamp_json"
                  remove_field => "[json_message][timestamp]"
                }
              }
          
              # Parsing structured data being part of the syslog message.
              # Example: "[instance@47450 director="" deployment="cf" group="api" az="z1" id="9b5c1d3c-a2fe-45b0-906e-fa88af93fd4a"]"
              if [syslog5424_sd] {
                kv {
                  # Convert the structured data into fields
                  source => "syslog5424_sd"
                  target => "sd"
                  remove_field => [ "syslog5424_sd" ]
                  trim_key => "\["
                  trim_value => "\]"
                }
              }
              date {
                match => [ "syslog5424_ts", "ISO8601" ]
                remove_field => [ "syslog5424_ts", "timestamp" ]
              }
          }
        input-10-syslog: |
          input {
              tcp {
                add_field => [ "@input", "syslog" ]
                id => "input-syslog/${JOB_FULL_AZ_DEPLOYMENT}"
                port => "5514"
              }
              #syslog {
              #    port => 5514
              #    use_labels => true
              #    type => "syslog"
              #    tags => []
              #    add_field => { "_index_name" => "syslog" }
              #}
          }
          
          
        input-20-dlq: |
          input {
            dead_letter_queue {
              path => "/var/vcap/store/logstash/plugins/inputs/dead_letter_queue"
              commit_offsets => true
              pipeline_id => "cf-platform-es"
            }
          }
        output-10-es: |
          output {
            elasticsearch {
              # DS_HOSTS is default from elasticsearch_master property
              hosts => [ "${DST_HOSTS}" ]
              user => "${ES_USER}"
              password => "${ES_PASSWORD}"
              sniffing => false
              index => "${ES_INDEX_PREFIX}-%{+YYYY.MM.dd}"
              http_compression => true
              manage_template => false
              id => "output-es/${JOB_FULL_AZ_DEPLOYMENT}"
            }
            # stdout { codec => rubydebug }
          }
        output-20-dlq: |
          output {
            elasticsearch {
              # DS_HOSTS is default from elasticsearch_master property
              hosts => [ "${DST_HOSTS}" ]
              user => "${ES_USER}"
              password => "${ES_PASSWORD}"
              sniffing => false
              index => "${ES_INDEX_PREFIX}-dlq-%{+YYYY.MM.dd}"
              http_compression => true
              manage_template => false
              id => "output-dlq/${JOB_FULL_AZ_DEPLOYMENT}"
            }
          }
