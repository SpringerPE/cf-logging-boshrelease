- type: replace
  path: /instance_groups/name=logstash/properties/logstash/conf?/xpack.management.pipeline.id?/-
  value:
    "cf-platform-es"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env?/DST_HOSTS?
  value:
    "((es-host))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/ES_USER?
  value:
    "((es-user))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/ES_PASSWORD?
  value:
    "((es-password))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/ES_INDEX_PREFIX?
  value:
    "((es-index_prefix))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/CF_API?
  value:
    "((cf-api))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/SOURCE_ENV?
  value:
    "((source-env))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/env/SOURCE_PLATFORM?
  value:
    "((source-platform))"

- type: replace
  path: /instance_groups/name=logstash/properties/logstash/pipelines?
  value:
    - name: cf-platform-es
      config:
        filter-00-prefiltering: |
          ##-------------
          # Pre filtering
          ##-------------
          
          filter {
              mutate {
                  # Replace the unicode empty character \u0000 with ""
                  gsub => [ "message", '\u0000', ""]
          
                  # Trim excess whitespace
                  strip => [ "message" ]
              }
          
              # Drop empty useless logs
              if [message] =~ /^\s*$/ or [message] =~ /^#.*$/ {
                  drop { }
              }
          
              mutate {
                  rename => { "message" => "@message" }
          
                  # Add tags to track which job processed this event
                  add_field => {
                          "[@parser][job]" => "${JOB_FULL_AZ_DEPLOYMENT}"
                          "[@parser][instance]" => "${INSTANCE_ID}"
                          "[@parser][az]" => "${JOB_AZ}"
                          "[@parser][name]" => "${JOB_NAME}"
                          "[@parser][index]" => "${JOB_INDEX}"
                          "[@parser][deployment]" => "${DEPLOYMENT_NAME}"
                          "[@parser][lb-host]" => "%{host}"
                          "[@parser][lb-port]" => "%{port}"
                  }
          
                  # When behind LB, this is always the IP of the haproxy, not the IP of the actual host sending the data.
                  # Remove it to avoid confusion
                  remove_field => [ "host", "port" ]
              }
          }
        filter-10-syslog_rfc5424_parsing: |
          filter {
              # Manually parse the log for RFC5424
              grok {
                break_on_match => true
                match => { "@message" => "%{SYSLOG5424LINE}" }
              }
          
              if [syslog5424_ts] {
                # Handle RFC5424 formatted Syslog messages
                mutate {
                  remove_field => [ "message", "host" ]
                  add_tag => [ "syslog5424" ]
                }
                mutate {
                  # Use a friendlier naming scheme
                  rename => {
                    "syslog5424_app"  => "program"
                    "syslog5424_msg"  => "message"
                    "syslog5424_host" => "host"
                  }
                  remove_field => [ "syslog5424_ver", "syslog5424_proc" ]
                }
          
                if [syslog5424_sd] {
                  # All structured data needs to be in format [key=value,key=value,...]
                  mutate {
                    # Remove wrapping brackets
                    gsub => [ "syslog5424_sd", "[\[\]]", "" ]
                  }
                  kv {
                    # Convert the structured data into Logstash fields
                    source => "syslog5424_sd"
                    field_split => ","
                    value_split => "="
                    remove_field => [ "syslog5424_sd" ]
                  }
                }
                date {
                  match => [ "syslog5424_ts", "ISO8601" ]
                  remove_field => [ "syslog5424_ts", "timestamp" ]
                }
              }
          }
        input-10-syslog: |
          input {
              tcp {
                add_field => [ "@input", "syslog" ]
                id => "input-syslog/${JOB_FULL_AZ_DEPLOYMENT}"
                port => "5514"
              }
              #syslog {
              #    port => 5514
              #    use_labels => true
              #    type => "syslog"
              #    tags => []
              #    add_field => { "_index_name" => "syslog" }
              #}
          }
          
          
        output-10-es: |
          output {
            elasticsearch {
              # DS_HOSTS is default from elasticsearch_master property
              hosts => [ "${DST_HOSTS}" ]
              user => "${ES_USER}"
              password => "${ES_PASSWORD}"
              sniffing => false
              index => "${ES_INDEX_PREFIX}-%{+YYYY.MM.dd}"
              http_compression => true
              manage_template => false
              id => "output-es/${JOB_FULL_AZ_DEPLOYMENT}"
            }
            # stdout { codec => rubydebug }
          }
